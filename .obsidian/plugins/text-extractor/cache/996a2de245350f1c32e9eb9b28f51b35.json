{"path":"PDF Library/Vendor/Calrec/NBC_Remote_equipment_room_proposal_v11.pdf","text":"RESTRICTED NBC NEW JERSEY SERVER CENTER Page 1 of 4 Technical considerations for Calrec equipment. Peter Walker, V1.2, 15 th Mar 2017. Overview A. To provide support for working with a remote equipment room, connected via the clients IP network. A number of Calrec processing cores and I/O boxes are to be physically located in a server facility in New Jersey, with control surfaces and more I/O located in a studio facility in New York’s Rockefeller Center. This new processing and I/O needs to connect with the existing Calrec Hydra2 network in Rockefeller Center. The intention is to expand on this over time, adding more remote cores, and connecting control surfaces and I/O from Rockefeller and other studio facilities. B. To provide support for switching processing cores between control surfaces. Allow the client to save on the number of high capacity DSP cores they require by being able to deploy them as and when needed across various control-rooms when there are large production demands, allowing smaller capacity DSP cores to be used for more productions. Control Surface Connections 1. To support loudness metering, the client’s IP network must allow forwarding of multicast UDP packets from cores to surfaces. 2. Other metering data is sent to control surfaces in LLC packets. It is expected that Calrec will need to do some work to package this data to allow it to be forwarded through the clients IP network. 3. Each system uses the same MAC addresses for its core-surface connections. Each connection therefore needs to be on its own VLAN to prevent conflict with the others. The primary and secondary connection of each system should also be on separate VLANs to prevent conflict. 4. It should be noted that the connections on the surface and core are not network end-points, they are in fact switches that distribute/aggregate data across multiple entities within the core/surface. 5. It is assumed that the client will use VLAN configuration manually, or via software defined networking, in order to switch surface to core connections. It is not proposed that Calrec hardware or software will instigate or control connection switchovers. 6. Control surfaces get their IP addresses allocated from a processing core on the first connection after boot up. After a core changeover, a control surface will need to be reset for it to get a new IP RESTRICTED NBC NEW JERSEY SERVER CENTER Page 2 of 4 address and be able to work with the new core. This will need to be a manual operation performed by pressing the reset button on the control surface. It is not proposed that Calrec will automate the re-allocation of surface IP addresses, or provide a remote or automated method for resetting surfaces. 7. Control surface PCs are not affected by surface resets. On connecting with a different core, the existing Calrec Windows service software will automatically update the IP address of the PC. The Calrec UI application will require a restart after the surface connects. 8. No data is stored on control surfaces themselves. All data, including user memories and surface panel layouts are stored within the processing core. For cores to work with different control surfaces, each surface will ideally have the same control panel layout, with the same underlying POE switch distribution to those panels, I.E. the physical panel to POE port connections should be the same for each surface. Processing cores can store multiple surface layouts, so surfaces that are not laid out the same can be used, however in such a case, the correct layout would need to be loaded after the core connects to the surface. Applying a layout change requires a surface reset, meaning that if switching a core between surfaces that are not of the same layout, the surface will need to be reset twice, first to reconnect with a new IP address, and again after loading the new surface layout. 9. As the user memories are also stored within the processing core, appropriate settings for each surface/control-room should be stored on each core. If required, user settings can be transferred between processing cores using existing backup/restore functionality. 10. The control surfaces in the proposal are all Artemis. The processing cores are all variants of Artemis, providing up to 3 different sized DSP packs that can be utilized by the Artemis surfaces. The switching of Apollo and Summa cores/surfaces is outside of the scope of this proposal. Hydra2 Audio and Control Connections 11. Calrec to provide a new product, project name “Hydra2-IP Link” that will allow Hydra2 connections to be extended over an IP network. The Hydra2-IP Link will allow a Hydra2 network to span large geographic distances without the overhead of dedicated dark fiber. 12. Once configured, Hydra2-IP Links are transparent to the audio operator - remotely connected I/O is available to them for use exactly as locally connected I/O is, including labelling, control capability, sharing and protection. All of the connected hardware is effectively sitting on a single Hydra2 network. 13. There must be a single processing core acting as the Master Router for the whole Hydra2 network at any given time. This Master Router can reside on any part of the network, but some consideration should be given to potential fault scenarios. Other processing cores can be configured to automatically promote in the event that connectivity with the master is lost. It is recommended that initially at least, the default Master Router remains within the existing Hydra2 network in Rockefeller Center. At some stage, as the New Jersey server center expands it will likely make sense to make one of the router cores there the default master. 14. The sync connection to all processing cores on a Hydra2 network, including those on opposite ends of an IP link, needs to be synchronous with each other. It is expected that the video/Wordclock/AES sync distribution system to the cores will likely need to be referenced from GPS, or a common PTP source. The H2-IP Links may operate in different PTP domains assuming they are GPS referenced. RESTRICTED NBC NEW JERSEY SERVER CENTER Page 3 of 4 15. Each Hydra2-IP Link unit is expected to be a 1U box, capable of transporting 512 channels of 48kHz audio in each direction. A single Hydra2 port connects each unit to a router card (not an “Expansion” card) fitted in a processing core. The units work in pairs, connected together via IP to form a 512 channel trunk link between processing cores. Audio is transported between the two units using the AES67 protocol. 16. Multiple pairs of Hydra2-IP Links can be used to form a trunk link of the required bandwidth between the two cores. In such cases, the router cards on either side need their trunk link configuration setting to prevent a network loop and to make use of the full bandwidth. 17. All pairs forming a given trunk link need to connect to a single core on either side. Trunk links between two sites cannot be spread across multiple cores. Up to 4096 audio channels can be passed between two cores and forwarded on. Additional trunk links between other cores could be put in place if required by ensuring that such cores are only interconnected on one side of the IP link. 18. This proposal does not allow Hydra2 I/O boxes to connect directly to a Hydra2-IP Link, I/O must physically connect via cores. 19. For each pair forming a link, a secondary pair can be connected between the secondary router cards in the same cores to provide redundancy. 20. Within each Hydra2-IP Link unit, there will be 2 x 1Gbs AoIP end points, each with a 256 channel capability. A 3rd IP connection will be used on each unit to pass Hydra2 control data over the IP link. 21. Initial config of each unit will require accessing the web-app served by each AoIP end-point within. This can be done over the AoIP network if the IP address of the AoIP end-points are known, or via the known address of the front panel config port. A static IP configuration will need to be applied to the Hydra2 control data port via the front panel configuration port. 22. It is expected that each AoIP end-point will be configured with 32 x 8 channel talker streams and 32 x listener streams that connect one-to-one with talker streams on the corresponding unit on the other side of the IP link. 23. This initial port and stream config will not need to change once set up. This config could be carried out at the factory prior to shipping. 24. Packet time and link offset for the streams will need to be set to be appropriate for the latency of the client’s network. Maximum packet time available will be 1ms. 25. The AoIP streams will be multicast. Calrec do not support unicast streaming at this stage. RESTRICTED NBC NEW JERSEY SERVER CENTER Page 4 of 4 Figure 1, proposed physical connectivity:","libVersion":"0.3.1","langs":""}