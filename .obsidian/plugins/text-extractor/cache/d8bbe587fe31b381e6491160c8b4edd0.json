{"path":"PDF Library/30 Rock Global Infrastructure/Devices and Architecture/7N_Device Descriptions.pdf","text":"Control Room Pod #1 – Racks 55‐57 Every 3 racks = 1 control room The design team is currently building 5 control  backends like the one above.  More detail on  each item on the next pages The top of rack FRP is where EXE QSFP‐ based fiber breaks out into individual  strands.  This allows maximum utilization of  each EXE port (in this rack it feeds the IPGs  below) This is the network access switch for this  rack and is part of the 7N Production  Network which is using Cisco ACI.  Some  devices are wired across multiple racks but  all end point connections can be found by  searching the wire database for “SW‐055A”  under source connection This device is converting from the  compressed video format NDI to ST‐2110  and uplinks to the EXE via the IPX in rack 56. While this device is labelled as a Viz Trio, it  may also act a virtual control room and  should be labelled as “GFX‐055A” to allow  for that flexibility.  Evertz 570 Frames (570FR) are the modular  frame with no door on the front.  The ones  in this rack house IPGs (570IPG‐X19‐25GE)  which feed the XVS‐8000 below   While the cards in these frames are all the  same model number, they are running  different apps to configure the DIN  connectors on the rear plane for different IO  configurations (8x8, 2x16) Frame labels are important because that is  how the frame controller is identified on the  network connection documents (FC‐055A) While the cards all have their own IP  address, they all connect to the  management network via the frame  controller  While the cards in these Sony Professional Workstation, this is the  same as in 71/72 but is the new generation Sony MKS, same as in 71/72 but is the new  model.  Feeds tally, machine control, and  name transfer via the 7800FR in the rack to  the right.   Sony XVS‐8000 Production Switcher, also  the same as 71/72 but this generation is IP‐ ready but the IO count is not sufficient with  those option boards so it wrapped around  the IPGs above.  Connection to the panel is  via the Cisco ACI network and should be  treated as separate items (i.e. any panel  could be connected to any tub with a  network config change) Standard Calrec Modular I/O box that is  console connected below.  This is providing  Waves IO to the Soundgrid servers in the 2  racks to the right (via the network) and  GPOI via a tally router in the 7800FR in the  rack to the right Calrec Artemis Core that is Hydra2 Core  connected.  All IO is via the H2 Core and the  TDM‐Hydra conversion cards in the 570FR in  the rack to the right.   As with the production switcher, the  operating theory is that the surface can be  swung to any core with a network  configuration change.   The top of rack FRP is where EXE QSFP‐ based fiber breaks out into individual  strands.  This allows maximum utilization of  each EXE port (in this rack it feeds the  570EMR‐AG‐HUB TDM conversion cards in  the 570FR) The switches in all control room racks are  identical and can be found in the wire  database for connection details.  CAD  drawings are also available in Vault.   Waves SoundGrid Extreme Server which is  only connected to the Production Network.   IO is from the Calrec Core and control is  from a virtual machine This 570FR houses the AES‐67 to TDM  conversion cards (570EMR‐AG‐HUB) and the  TDM to Hydra 2 conversion cards (570EMR‐ HYDRA) which take the place of the Calrec  MUX and DeMUX cards in other control  rooms.   7800 frames are black  and nearly identical  to 570FRs but they have a front door that  must be closed.   This frame houses tally routers that provide  GPIO to the XVS and Calrec in the rack to  the left, a protocol translation card for  name transfer to the XVS and NAT  translation cards for the graphics servers.   Dreamcatcher is taking the place of MIRA in  these control rooms as the DDR playout  device.  Each of these devices is 4 channels  of playout and has no inputs or local  storage.  They connect to a core of  Dreamcatcher servers that perform ingest  (from the EXE) and file transcodes which  creates a shared file system accessible from  all control rooms and playout servers.   Edge frames are 6RU modular frames with  no front door.  They have redundant frame  controllers (1 on each side at the bottom of  the frame) This frame houses (2) IPX‐48 aggregation  switches which allow 10G end points to  connect to the 25G ports on the EXE more  efficiently.  These switches connect the  evBLADE cards, DreamCatcher, Graphics,  and NDI converter   The evBLADE cards perform a multitude of  processing tasks such as up/down/cross  conversion, color correction, frame sync,  profanity delay, ANC trigger insertion/ generation and decoding, with more  services under development.  These are  replacing the dedicated cards.   NOTE: each card has 4 processing cores,  each with an uplink to the X and Y switches  so there are 8 SFPs on each card.   NOTE ON IPX Aggregation switches:  There are 2 classes of device in this system,  core and edge.  Core devices are the EXEs  and IPXs and each can only pass X or Y  traffic so each EXE requires an IPX.   Edge devices connect to both cores for  redundancy (many support hitless merge or  at least stream failover).  Devices feeding  sources to the routers feed both X and Y.   These 2 frames are multiviewers (evMV, but  we call them VIPs to align with existing  nomenclature).   Each frame contains (5) VIPs and (1) IPX.   The VIPs are wired to both IPXs to achieve  the X/Y separation mentioned on the left.   The VIPs have different licenses based on IO  count and are wired to support those  channel counts (either 2 or 3 QSFPs to each  IPX)  There are several models of IPX, the ones in  EDGE‐A are IPX‐48s and 48 ports in the form  of (12) QSFPs located on the front.   IPX‐128s and MVIPR1Ts have 128 ports  presented as (12) QSFPs on the front and  (20) on the rear.  The difference is the  number of 25/100G ports, indicated by a  black background and white numbering (the  IPX‐128 has more) These are graphics servers that will be  provisioned as with VIZ or Ross Xpression  depending on production requirements.   These uplink to the IPX above via the  7880IPG‐NAT‐6 in the 7800FR above.  The  reason for NAT translation is that VIZ cannot  tune multicasts from Evertz without  restarting (impacts video returns to the  engine) No devices in this rack go thru the FRP.  All  connections that are using all strands of an  MPO are direct connections (i.e. IPX uplinks) The 9821EMR‐DANTE converts AES‐67 audio  to and from Dante.  The new 7N intercom is  Dante based and this will feed mix/minus &  program to that system.   Redundant server to the one on the right.  This frame is currently empty and will be  used for future expansion or 4k capabilities.   This is a copy of EDGE‐056A but does not  uplink to the NDI converter in rack 55.   Each control room has 16 channels of  playout.  These devices are connected  to  the IPXs in EDGE‐B.  There is a single line  drawing in Vault for all these connections These frames are identical to EDGE‐056B  and EDGE‐056C.  The wiring is also the same  but the licenses may vary.  The router IO is  based on this and in the event of failure, can  be found on the drawing and Evertz should  have it recorded and will provide an RMA  replacement with the same licenses.   Additional graphics playout (each control  room has 6 total servers).  These pass thru  the NAT cards in rack to the left but uplink  to the EXEs via the IPXs in EDGE‐057B 7800FR handling reference distribution.   There are 3 reference feeds from the master  distribution in rack 79 (analogous to the 2nd  floor GPS connected system).  The first 2  feeds all frames and graphics devices, the  3 rd feeds only the Sony switcher.   Shared Services – 2 Rack groups from rack 37‐46 Each 2‐rack POD is identical.  Currently only racks 37‐38 and  45‐46 are being built out, the rest will be phased.  These  devices feed the shared video, robo, and graphics areas FRPs in these racks are feeding the IPGs  which are wrapped around the CCUs. The edge frames each contain an IPX which  connects to end points in both frames in  each group for X/Y diversity  Odd‐numbered racks have (4) evBLADE  processors which are identical to the ones in  the CR Pods but may have different licenses.  As with the rack on the left, this frame  contains an IPX which is cross wired to both  frames.   Even‐numbered racks have (5) VIPs which  are configured in a 36x4 license and all  receive 3x QSFP connections to each IPX.   These are video, robo, and graphics  multiviewer prcoessors.  As with the TRIO in rack 55, this may be  used for virtual control rooms or  applications requiring GPU acceleration.   Generic naming is again required.   Each shared service pair has (6) graphics  playout servers which supplement those in  the CR Pods for elections and other special  events.  They all connect to 7880IPG‐NAT‐6  cards in the 7800FR in the odd‐racks.   These IPGs are the HD IO for the CCUs in  both racks which will be more relevant in  the future when certain productions are  using 4k.  All cards are 8x8 and identical to  those in rack 55.   As with the FR‐B on the left, this is the 4K IO  for the CCUs in the even numbered rack.   This frame houses a tally router (feeding  tally to the CCUs in both racks), reference  DAs for devices in both racks (2 feeds from  rack 79), and the 7880IPG‐NAT‐6 cards for  the graphics servers.   This frame is connected to the CCUs in the  odd‐numbered rack and provides quad‐link  4k input and output to each CCU.  Initially  these will be HD only but are easily  converted.   There will be (8) CCUs in each rack, all are  the new model Sony has demo’d in Studio  6A.  The video IO is wired to the IPGs above  and the SMPTE connection to the 4 th floor is  via the SMPTE fiber switch below.   There is a SMPTE fiber switch shared  between every (2) CCUs which provides a  4x1 functionality for each CCU.  These  switches will be control via Magnum and  the inputs are fed from the 4S camera patch  facility.","libVersion":"0.3.1","langs":""}